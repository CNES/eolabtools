{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b38435c-d361-4397-9ec3-ac0fed863b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/6/21 11:00:00\n",
      "153:11:46.3 62:34:11.0\n",
      "Azimuth of the sun: 153.19618436604858\n",
      "Elevation of the sun: 62.56973057127518\n"
     ]
    }
   ],
   "source": [
    "import ephem\n",
    "import math\n",
    "\n",
    "\n",
    "# Set the geographical coordinates of the location where you want to perform the calculation\n",
    "latitude = '48.8566'  # Latitude of Paris\n",
    "longitude = '2.3522'  # Longitude of Paris\n",
    "\n",
    "\n",
    "# Create an object to represent the sun\n",
    "sun = ephem.Sun()\n",
    "\n",
    "# Set the date and time for which you want to calculate the azimuth and elevation\n",
    "date = ephem.Date(\"2023/06/21 11:00:00\")\n",
    "print(date)\n",
    "# Create an observer object from the geographical coordinates\n",
    "observer = ephem.Observer()\n",
    "observer.lat = latitude\n",
    "observer.lon = longitude\n",
    "\n",
    "# Set the date and time for the observer\n",
    "observer.date = date\n",
    "\n",
    "# Calculate the azimuth and elevation of the sun\n",
    "sun.compute(observer)\n",
    "\n",
    "# Get the calculated values in radians\n",
    "azimuth_rad = sun.az\n",
    "elevation_rad = sun.alt\n",
    "print(azimuth_rad, elevation_rad)\n",
    "# Convert radians to degrees\n",
    "azimuth_deg = math.degrees(azimuth_rad)\n",
    "elevation_deg = math.degrees(elevation_rad)\n",
    "\n",
    "# Display the results\n",
    "print(\"Azimuth of the sun:\", azimuth_deg)\n",
    "print(\"Elevation of the sun:\", elevation_deg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f74ac8b6-ff7e-41df-9249-c2647e43838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.8534951 2.3483915\n",
      "2023/6/21 10:00:00\n",
      "128:50:06.7 56:19:30.4\n",
      "Azimuth of the sun: 128.83520820055256\n",
      "Elevation of the sun: 56.3251208395643\n"
     ]
    }
   ],
   "source": [
    "# Create a geocoder object\n",
    "geolocator = Nominatim(user_agent=\"my_app\")\n",
    "\n",
    "# Get the location of Paris\n",
    "location = geolocator.geocode('Paris')\n",
    "\n",
    "# Extract the latitude and longitude\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print(latitude, longitude)\n",
    "\n",
    "# Create an object to represent the sun\n",
    "sun = ephem.Sun()\n",
    "\n",
    "# Set the date and time for which you want to calculate the azimuth and elevation\n",
    "date = ephem.Date(\"2023/06/21 10:00:00\")\n",
    "print(date)\n",
    "# Create an observer object from the geographical coordinates\n",
    "observer = ephem.Observer()\n",
    "observer.lat = str(latitude)\n",
    "observer.lon = str(longitude)\n",
    "\n",
    "# Set the date and time for the observer\n",
    "observer.date = date\n",
    "\n",
    "# Calculate the azimuth and elevation of the sun\n",
    "sun.compute(observer)\n",
    "\n",
    "# Get the calculated values in radians\n",
    "azimuth_rad = sun.az\n",
    "elevation_rad = sun.alt\n",
    "print(azimuth_rad, elevation_rad)\n",
    "# Convert radians to degrees\n",
    "azimuth_deg = math.degrees(azimuth_rad)\n",
    "elevation_deg = math.degrees(elevation_rad)\n",
    "\n",
    "# Display the results\n",
    "print(\"Azimuth of the sun:\", azimuth_deg)\n",
    "print(\"Elevation of the sun:\", elevation_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dd77761-781a-4e75-a7eb-38a05fe76b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21 10:00:00+00:00\n",
      "2023-06-21 10:00:00\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the timezone for Paris\n",
    "paris_tz = pytz.timezone('Europe/Paris')\n",
    "\n",
    "# Create a datetime object in the Paris timezone\n",
    "paris_dt = paris_tz.localize(datetime(2023, 6, 21, 12, 0, 0))\n",
    "\n",
    "# Convert the datetime to UTC\n",
    "utc_dt = paris_dt.astimezone(pytz.utc)\n",
    "print(utc_dt)\n",
    "# Format the UTC datetime\n",
    "formatted_utc_dt = utc_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Print the formatted UTC datetime\n",
    "print(formatted_utc_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "381a165c-437b-47fa-bfe6-e74a6cf63783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution in X direction: 0.5 meters\n",
      "Resolution in Y direction: 0.5 meters\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# Open the TIFF file\n",
    "dataset = gdal.Open('/work/EOLAB/DATA/MNS/Paris/75-2021-0643-6862-LA93-0M50.tif')\n",
    "\n",
    "# Get the geotransform information\n",
    "geotransform = dataset.GetGeoTransform()\n",
    "\n",
    "# Extract the pixel size in X and Y directions\n",
    "pixel_size_x = geotransform[1]\n",
    "pixel_size_y = geotransform[5]\n",
    "\n",
    "# Get the spatial reference system\n",
    "srs = dataset.GetProjection()\n",
    "\n",
    "# Close the dataset\n",
    "dataset = None\n",
    "\n",
    "# Print the resolution in meters\n",
    "print(\"Resolution in X direction:\", abs(pixel_size_x), \"meters\")\n",
    "print(\"Resolution in Y direction:\", abs(pixel_size_y), \"meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623cda54-e22e-4c9c-8198-0b0b75e5cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime('2023-05-31', \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime('2023-06-10', \"%Y-%m-%d\")\n",
    "step = timedelta(days=int(1))\n",
    "\n",
    "# Generate the range of dates\n",
    "dates = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd34bc86-2f57-44cc-8021-a602299c29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-01 09:00:00\n",
      "2023-01-01 12:30:00\n",
      "2023-01-01 15:45:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define the date and list of times\n",
    "date = datetime(2023, 1, 1)\n",
    "times = ['09:00', '12:30', '15:45']\n",
    "\n",
    "# Create a list of datetime objects\n",
    "datetime_list = []\n",
    "for time in times:\n",
    "    time_obj = datetime.strptime(time, '%H:%M')\n",
    "    datetime_obj = datetime(date.year, date.month, date.day, time_obj.hour, time_obj.minute)\n",
    "    datetime_list.append(datetime_obj)\n",
    "\n",
    "# Print the list of datetime objects\n",
    "for dt in datetime_list:\n",
    "    print(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7061e466-7134-4230-b6aa-3a823e4862be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:00\n",
      "09:30\n",
      "10:00\n",
      "10:30\n",
      "11:00\n",
      "11:30\n",
      "12:00\n",
      "12:30\n",
      "13:00\n",
      "13:30\n",
      "14:00\n",
      "14:30\n",
      "15:00\n",
      "15:30\n",
      "16:00\n",
      "16:30\n",
      "17:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the start time, end time, and step\n",
    "start_time = '09:00'\n",
    "end_time = '17:00'\n",
    "step = 30\n",
    "\n",
    "# Parse the start time and end time\n",
    "start_datetime = datetime.strptime(start_time, '%H:%M')\n",
    "end_datetime = datetime.strptime(end_time, '%H:%M')\n",
    "\n",
    "# Generate the list of times\n",
    "time_list = []\n",
    "current_datetime = start_datetime\n",
    "while current_datetime <= end_datetime:\n",
    "    time_list.append(current_datetime.strftime('%H:%M'))\n",
    "    current_datetime += timedelta(minutes=step)\n",
    "\n",
    "# Print the list of times\n",
    "for time in time_list:\n",
    "    print(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50f1a81-bbf8-477b-b618-886772db20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 48.8534951\n",
      "Longitude: 2.3483915\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Create a geocoder object\n",
    "geolocator = Nominatim(user_agent=\"my_app\")\n",
    "\n",
    "# Get the location of Paris\n",
    "location = geolocator.geocode(\"Paris\")\n",
    "\n",
    "# Extract the latitude and longitude\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "\n",
    "# Print the latitude and longitude\n",
    "print(\"Latitude:\", latitude)\n",
    "print(\"Longitude:\", longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98abfe3d-a5af-49fd-8847-4db0bf8450ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75-2021-0643-6862-LA93-0M50'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.splitext(os.path.basename('/work/EOLAB/DATA/MNS/Paris/75-2021-0643-6862-LA93-0M50.tif'))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bada808-3c39-48d2-9d13-27f861031204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# List of files\n",
    "file_list = glob.glob(\"/work/EOLAB/DATA/MNS/Paris/*.tif\")\n",
    "\n",
    "# Output file path\n",
    "output_file = \"listing.lst\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Iterate over the file list\n",
    "    for file in file_list:\n",
    "        # Get the absolute path of each file\n",
    "        file_path = os.path.abspath(file)\n",
    "        # Write the file path to the output file\n",
    "        f.write(file_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab3595c-61bd-4dbe-8a02-355f79602c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mosaic-hillshade-20230531-1530.tif\n",
      "['outputs/75-2021-0647-6861-LA93-0M50-hillshade-20230531-1530.tif', 'outputs/75-2021-0651-6865-LA93-0M50-hillshade-20230531-1530.tif', 'outputs/75-2021-0643-6862-LA93-0M50-hillshade-20230531-1530.tif', 'outputs/75-2021-0647-6862-LA93-0M50-hillshade-20230531-1530.tif', 'outputs/75-2021-0653-6866-LA93-0M50-hillshade-20230531-1530.tif']\n"
     ]
    }
   ],
   "source": [
    "import gdal\n",
    "# Output mosaic file\n",
    "\n",
    "files = glob.glob(\"outputs/*1530*\")\n",
    "\n",
    "output_file = \"mosaic-\" + files[0][-27:]\n",
    "print(output_file)\n",
    "\n",
    "print(files)\n",
    "\n",
    "g = gdal.Warp(output_file, files, format='GTIFF')\n",
    "g = None\n",
    "# Close the mosaic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ad8a65-fa78-49a4-a6ca-cc71eff59569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2983'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "error_message = \"ValueError: The radius (option --radius, value=2983) must be strictly less than half the size of the window (option --window_size, value=1024)\"\n",
    "re.search(r\"value=(\\d+)\", error_message).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd998e5-b0c8-4260-a559-b319a20f3c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "646 < (1024 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29932bd6-7a52-4077-9262-70fe5bde6b25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700137800.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create a datetime object\n",
    "dt = datetime(2023, 11, 16, 12, 30, 0)\n",
    "\n",
    "# Convert the datetime to a float timestamp\n",
    "timestamp = dt.timestamp()\n",
    "print(timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51099a-e5b6-41b0-b7c7-60e65ec8690f",
   "metadata": {},
   "source": [
    "# Obtenir mask ombre soleil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2d706f-3124-49a7-99ed-e9f704e48ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717160400\n",
      "mask time 2024-05-31 13:00:00 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "1717162200\n",
      "mask time 2024-05-31 13:30:00 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "1717164000\n",
      "mask time 2024-05-31 14:00:00 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "1717165800\n",
      "mask time 2024-05-31 14:30:00 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "1717167600\n",
      "mask time 2024-05-31 15:00:00 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[1.7171604e+09 1.7171604e+09 1.7171604e+09 ...           nan\n",
      "            nan           nan]\n",
      " [1.7171676e+09 1.7171604e+09 1.7171604e+09 ...           nan\n",
      "            nan           nan]\n",
      " [1.7171604e+09 1.7171604e+09 1.7171604e+09 ...           nan\n",
      "            nan           nan]\n",
      " ...\n",
      " [1.7171604e+09 1.7171604e+09 1.7171604e+09 ... 1.7171604e+09\n",
      "  1.7171604e+09 1.7171604e+09]\n",
      " [1.7171604e+09 1.7171604e+09 1.7171604e+09 ... 1.7171604e+09\n",
      "  1.7171604e+09 1.7171604e+09]\n",
      " [1.7171604e+09 1.7171604e+09 1.7171604e+09 ... 1.7171604e+09\n",
      "  1.7171604e+09 1.7171604e+09]]\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# List of paths to your input rasters\n",
    "input_raster_paths = [\"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1300.tif\", \n",
    "                      \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1330.tif\",\n",
    "                      \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1400.tif\",\n",
    "                      \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1430.tif\",\n",
    "                      \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1500.tif\"]\n",
    "\n",
    "times = [datetime.strptime(date_string[-17:-4], \"%Y%m%d-%H%M\") for date_string in input_raster_paths]\n",
    "timestamps = [int(time.timestamp()) for time in times]\n",
    "\n",
    "with rasterio.open(input_raster_paths[0]) as src:\n",
    "    height = src.height\n",
    "    width = src.width\n",
    "    profile = src.profile\n",
    "\n",
    "# Initialize variables to store the time of changes\n",
    "from_1_to_0 = np.full((height, width), np.nan)  # Initialize with infinity\n",
    "from_0_to_1 = np.full((height, width), np.nan)  # Initialize with infinity\n",
    "previous_mask = np.full((height, width), 1.) # Initialize to dark\n",
    "\n",
    "# Iterate through the input rasters\n",
    "for path, time, timestamp in zip(input_raster_paths, times, timestamps):\n",
    "    print(timestamp)\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)  # Read the raster data\n",
    "        print(\"mask time\", time, data)\n",
    "        print('\\n')\n",
    "        mask_changes_1_to_0 = (previous_mask == 1.) & (data == 0.)  # Identify pixels changing from 1 to 0\n",
    "        mask_changes_0_to_1 = (previous_mask == 0.) & (data == 1.)  # Identify pixels changing from 0 to 1\n",
    "        from_1_to_0[mask_changes_1_to_0] = timestamp # Update time for pixels changing from 1 to 0\n",
    "        from_0_to_1[mask_changes_0_to_1] = timestamp # Update time for pixels changing from 0 to 1\n",
    "        \n",
    "        previous_mask = data\n",
    "\n",
    "print(from_1_to_0)\n",
    "# # Create new rasters to store the time of changes\n",
    "with rasterio.open('from_1_to_0.tif', 'w', **profile) as dst:\n",
    "    dst.write(from_1_to_0, 1)  # Write the time of changes from 1 to 0 to the new raster\n",
    "with rasterio.open('from_0_to_1.tif', 'w', **profile) as dst:\n",
    "    dst.write(from_0_to_1, 1)  # Write the time of changes from 0 to 1 to the new raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c499c013-ec34-4d14-b8bb-a4ba44af902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_to_shadow = gpd.read_file('sun_to_shadow.gpkg', layer='sun_to_shadow')\n",
    "shadow_to_sun = gpd.read_file('shadow_to_sun.gpkg', layer='shadow_to_sun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3894be33-7d7f-4738-b078-f008055b519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/geodataframe.py:1803: FutureWarning: `unary_union` returned None due to all-None GeoSeries. In future, `unary_union` will return 'GEOMETRYCOLLECTION EMPTY' instead.\n",
      "  merged_geom = block.unary_union\n",
      "/tmp/slurm-2493866/ipykernel_229211/3722783468.py:1: UserWarning: `keep_geom_type=True` in overlay resulted in 47997 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  intersection = gpd.overlay(sun_to_shadow, shadow_to_sun, how='intersection')\n"
     ]
    }
   ],
   "source": [
    "intersection = gpd.overlay(sun_to_shadow, shadow_to_sun, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58128853-b124-4ac2-b70e-b53db8cb8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection.to_file('intersections.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c373e623-5f40-4aab-8cf4-f7f04814bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_paths = [\"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1300.tif\", \n",
    "              \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1330.tif\",\n",
    "              \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1400.tif\",\n",
    "              \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1430.tif\",\n",
    "              \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1500.tif\"]\n",
    "              # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1530.tif\",\n",
    "              # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1600.tif\",\n",
    "              #   \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1630.tif\",\n",
    "              #   \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1700.tif\"]\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1730.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1800.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1830.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1900.tif\"]\n",
    "raster_arrays = []\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_arrays.append(src.read(1))\n",
    "\n",
    "# Initialize counters for changes from 1 to 0 and 0 to 1\n",
    "changes_1_to_0 = np.zeros_like(raster_arrays[0])\n",
    "changes_0_to_1 = np.zeros_like(raster_arrays[0])\n",
    "\n",
    "# Compare corresponding pixels across the arrays to identify changes\n",
    "for i in range(len(raster_arrays) - 1):\n",
    "    changes_1_to_0 += np.logical_and(raster_arrays[i] == 1, raster_arrays[i + 1] == 0)\n",
    "    changes_0_to_1 += np.logical_and(raster_arrays[i] == 0, raster_arrays[i + 1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fa31eb-c894-4ceb-ac42-268112513dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.], dtype=float32), array([3667279,  330050,    2671]))\n",
      "(array([0., 1., 2.], dtype=float32), array([3699796,  290402,    9802]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(changes_0_to_1, return_counts=True))\n",
    "print(np.unique(changes_1_to_0, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36bbb76-84a6-4bf4-a0c4-24bba36ca6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, s = np.unique(changes_0_to_1 + changes_1_to_0, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ce78b7-4dea-494b-96a9-78f080877add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1717160400, 1717162200, 1717164000, 1717165800, 1717167600]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d0cd699-0494-4502-831c-eeec56715b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_0_to_1 + changes_1_to_0 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d4197c-89b2-4e5c-8f93-324c515fb043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_sun = np.full((height, width), 0.)  # Initialize with infinity\n",
    "end_sun = np.full((height, width), 0.)  # Initialize with infinity\n",
    "# previous_mask = np.full((height, width), 1.) # Initialize to dark\n",
    "\n",
    "# Iterate through the input rasters\n",
    "for path,timestamp in zip(raster_paths,timestamps):\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)  # Read the raster data\n",
    "        mask_start_sun = (start_sun == 0.) & (data == 0.)  # Identify pixels changing from 1 to 0\n",
    "        mask_end_sun = (start_sun != 0.) & (data == 1.) & (end_sun == 0.) # Identify pixels changing from 0 to 1\n",
    "        start_sun[mask_start_sun] = timestamp # Update time for pixels changing from 1 to 0\n",
    "        end_sun[mask_end_sun] = timestamp # Update time for pixels changing from 0 to 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43283874-6bfb-4331-8c69-197840af7683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0000000e+00, 1.7171604e+09, 1.7171622e+09, 1.7171640e+09,\n",
       "        1.7171658e+09, 1.7171676e+09]),\n",
       " array([ 273211, 3445764,  102024,   61822,   40113,   77066]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(start_sun, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540e148f-8695-4212-8f81-8ff3ce138e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0000000e+00, 1.7171622e+09, 1.7171640e+09, 1.7171658e+09,\n",
       "        1.7171676e+09]),\n",
       " array([3667279,   39289,   49969,   98960,  144503]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(end_sun, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e97b6a-6a52-478b-95bc-0cc88abd214d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_and(start_sun > end_sun,end_sun != 0.).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f014d9-62ef-4e07-9e3f-28fc0d493523",
   "metadata": {},
   "source": [
    "A faire : \n",
    "- créer deux raster avec les timestamp (0, times et nodata pour les valeurs non valides (+ de 2 changements)\n",
    "- Les passer en vecteurs (voir si ya pas trop de géometries dispatchées)\n",
    "- Les merger et checker la cohérence des deux timestamp (start and en sun)\n",
    "- Vérifier la cohérence et si ya toujours suffisamment de vecteurs dans la totalité de la zone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bc2de-10de-47dc-aeb7-f8284f6e165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import numpy as np \n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from osgeo import gdal, ogr, osr\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def coded_raster(stacked_array):\n",
    "    # reshape array\n",
    "    data_2d = stacked_array.reshape(-1, 4)\n",
    "\n",
    "    # get quadruples\n",
    "    unique_quadruplets, _ = np.unique(data_2d, axis=0, return_inverse=True)\n",
    "        \n",
    "    # create dict\n",
    "    quadruplet_dict = {tuple(q): i + 1 for i, q in enumerate(unique_quadruplets)}\n",
    "        \n",
    "    # create coded raster\n",
    "    new_array = np.array([[quadruplet_dict[tuple(stacked_array[i, j, :])] for j in range(stacked_array.shape[1])] for i in range(stacked_array.shape[0])])\n",
    "\n",
    "    return new_array, quadruplet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1009129-54e7-4bff-9fbe-cc003bf1266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_paths = sorted(glob.glob(\"../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-*.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91e689ee-120a-43f1-985c-a0a6ca3a5822",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-0800.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-0830.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-0900.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-0930.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1000.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1030.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1100.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1130.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1200.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1230.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1300.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1330.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1400.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1430.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1500.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1530.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1600.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1630.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1700.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1730.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1800.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1830.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1900.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1930.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2000.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2030.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2100.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2130.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2200.tif',\n",
       " '../outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-2230.tif']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raster_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abec1561-0539-4a84-8247-58ae1195b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_paths = raster_paths[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0f9e69f-ce55-40d1-aa86-2895452f184c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raster_paths = [\"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1300.tif\", \n",
    "#               \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1330.tif\",\n",
    "#               \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1400.tif\",\n",
    "#               \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1430.tif\",\n",
    "#               \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1500.tif\"]\n",
    "              # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1530.tif\",\n",
    "              # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1600.tif\",\n",
    "              #   \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1630.tif\",\n",
    "              #   \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1700.tif\"]\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1730.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1800.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1830.tif\",\n",
    "                # \"outputs/75-2021-0647-6863-LA93-0M50-hillshade-20240531-1900.tif\"]\n",
    "                \n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    height = src.height\n",
    "    width = src.width\n",
    "    profile = src.profile\n",
    "\n",
    "times = [datetime.strptime(date_string[-17:-4], \"%Y%m%d-%H%M\") for date_string in raster_paths]\n",
    "timestamps = [int(time.timestamp()) for time in times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68172cd-9588-46c6-85a9-200111f05339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# occurrence des changements\n",
    "\n",
    "raster_arrays = []\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_arrays.append(src.read(1))\n",
    "\n",
    "# Initialize counters for changes from 1 to 0 and 0 to 1\n",
    "changes_1_to_0 = np.zeros_like(raster_arrays[0])\n",
    "changes_0_to_1 = np.zeros_like(raster_arrays[0])\n",
    "\n",
    "# Compare corresponding pixels across the arrays to identify changes\n",
    "for i in range(len(raster_arrays) - 1):\n",
    "    changes_1_to_0 += np.logical_and(raster_arrays[i] == 1, raster_arrays[i + 1] == 0)\n",
    "    changes_0_to_1 += np.logical_and(raster_arrays[i] == 0, raster_arrays[i + 1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3c7a2e-568e-47c0-bdb7-d8297a055a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create 4 rasters\n",
    "\n",
    "first_sun = np.full((height, width), 0.)  # Initialize with infinity\n",
    "first_shadow = np.full((height, width), 0.)  # Initialize with infinity\n",
    "second_sun = np.full((height, width), 0.)\n",
    "second_shadow = np.full((height, width), 0.)\n",
    "# previous_mask = np.full((height, width), 1.) # Initialize to dark\n",
    "\n",
    "# Iterate through the input rasters\n",
    "for path,timestamp in zip(raster_paths,timestamps):\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)  # Read the raster data\n",
    "        mask_first_sun = (first_sun == 0.) & (data == 0.)  # Identify pixels changing from 1 to 0\n",
    "        mask_first_shadow = (first_shadow == 0.) & (first_sun != 0.) & (data == 1.)  # Identify pixels changing from 0 to 1\n",
    "        mask_second_sun = (second_sun == 0.) & (first_shadow != 0.) & (data == 0.)  # Identify pixels changing from 0 to 1\n",
    "        mask_second_shadow = (second_shadow == 0.) & (second_sun != 0.) & (data == 1.)  # Identify pixels changing from 0 to 1\n",
    "        \n",
    "        \n",
    "        first_sun[mask_first_sun] = timestamp # Update time for pixels changing from 1 to 0\n",
    "        first_shadow[mask_first_shadow] = timestamp # Update time for pixels changing from 0 to 1\n",
    "        second_sun[mask_second_sun] = timestamp # Update time for pixels changing from 1 to 0\n",
    "        second_shadow[mask_second_shadow] = timestamp # Update time for pixels changing from 0 to 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8943e4a5-3b33-4a18-a50d-c09e04a8bc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = changes_0_to_1 + changes_1_to_0 > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a56788-3770-4df8-bf5a-0479117deec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum() / mask.size *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a51078-17f7-460a-a1b4-a97bd07d35ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set pixels with too much changes to NaN\n",
    "first_sun[mask] = -1\n",
    "first_shadow[mask] = -1\n",
    "second_sun[mask] = -1\n",
    "second_shadow[mask] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158862ef-43fd-4c37-b560-911fa8cd5799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack rasters for poligonization\n",
    "stacked_rasters = np.stack([first_sun, first_shadow, second_sun, second_shadow], axis=2)\n",
    "\n",
    "coded_raster, dictionnary = coded_raster(stacked_rasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef34fd3-0c2c-467f-beaa-d3c01be0e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raster\n",
    "profile['dtype'] = 'int32'\n",
    "with rasterio.open('coded_raster.tif', 'w', **profile) as dst:\n",
    "    dst.write(coded_raster, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd85a57d-165b-409a-bebb-fddee7c81365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile['dtype'] = 'float64'\n",
    "with rasterio.open('first_sun_timestamp.tif', 'w', **profile) as dst:\n",
    "    dst.write(first_sun, 1)\n",
    "with rasterio.open('first_shadow_timestamp.tif', 'w', **profile) as dst:\n",
    "    dst.write(first_shadow, 1)\n",
    "with rasterio.open('second_sun_timestamp.tif', 'w', **profile) as dst:\n",
    "    dst.write(second_sun, 1)\n",
    "with rasterio.open('second_shadow_timestamp.tif', 'w', **profile) as dst:\n",
    "    dst.write(second_shadow, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa7dce8-fa67-4703-82ef-0e4c426dc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read coded geoms\n",
    "chunks = gpd.read_file(\"coded_geoms.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf1cc6a-302b-41db-a2cf-6f054faff02e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: A geometry of type POLYGON is inserted into layer SELECT of geometry type MULTIPOLYGON, which is not normally allowed by the GeoPackage specification, but the driver will however do it. To create a conformant GeoPackage, if using ogr2ogr, the -nlt option can be used to override the layer geometry type. This warning will no longer be emitted for this combination of layer and feature geometry type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317.83913397789\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "t1 = time.time()\n",
    "command = 'ogr2ogr output_dissolved.gpkg coded_geoms.gpkg -dialect sqlite -sql \"SELECT ST_Union(geom), code FROM coded_geoms GROUP BY code\" -f \"GPKG\"'\n",
    "subprocess.run(command, shell=True)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2146753a-b9b6-41fc-981d-5ac4a8247f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499.4431722164154\n"
     ]
    }
   ],
   "source": [
    "# dissolve geometries\n",
    "t1 = time.time()\n",
    "chunks.dissolve('code').to_file('coded_geoms_dissolved.gpkg')\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33542bef-6246-4cfd-afa1-8c4ea0828531",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "code: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"fiona/ogrext.pyx\", line 136, in fiona.ogrext.gdal_open_vector\n  File \"fiona/_err.pyx\", line 291, in fiona._err.exc_wrap_pointer\nfiona._err.CPLE_OpenFailedError: code: No such file or directory\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/tmp/slurm-4599257/ipykernel_485006/670526082.py\", line 11, in read_geopackage_chunk\n    return gpd.read_file(chunk)\n  File \"/home/ad/duthoit/.local/lib/python3.10/site-packages/geopandas/io/file.py\", line 297, in _read_file\n    return _read_file_fiona(\n  File \"/home/ad/duthoit/.local/lib/python3.10/site-packages/geopandas/io/file.py\", line 338, in _read_file_fiona\n    with reader(path_or_bytes, **kwargs) as features:\n  File \"/home/ad/duthoit/.local/lib/python3.10/site-packages/fiona/env.py\", line 457, in wrapper\n    return f(*args, **kwds)\n  File \"/home/ad/duthoit/.local/lib/python3.10/site-packages/fiona/__init__.py\", line 292, in open\n    colxn = Collection(\n  File \"/home/ad/duthoit/.local/lib/python3.10/site-packages/fiona/collection.py\", line 243, in __init__\n    self.session.start(self, **kwargs)\n  File \"fiona/ogrext.pyx\", line 588, in fiona.ogrext.Session.start\n  File \"fiona/ogrext.pyx\", line 143, in fiona.ogrext.gdal_open_vector\nfiona.errors.DriverError: code: No such file or directory\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(read_geopackage_chunk, chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Wait for all tasks to complete\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     gdfs \u001b[38;5;241m=\u001b[39m [future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate the GeoDataFrames obtained from different chunks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m result_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mconcat(gdfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[44], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(read_geopackage_chunk, chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Wait for all tasks to complete\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     gdfs \u001b[38;5;241m=\u001b[39m [\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate the GeoDataFrames obtained from different chunks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m result_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mconcat(gdfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mDriverError\u001b[0m: code: No such file or directory"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"coded_geoms.gpkg\"\n",
    "\n",
    "# Define the number of processes to use\n",
    "num_processes = 4  # Adjust as needed based on your available resources\n",
    "\n",
    "def read_geopackage_chunk(chunk):\n",
    "    return gpd.read_file(chunk)\n",
    "\n",
    "# Read the GeoPackage file in chunks\n",
    "chunks = gpd.read_file(file_path, chunksize=1000)  # Adjust the chunksize as needed\n",
    "\n",
    "# Use multiprocessing to parallelize the read operation\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    # Submit read operations for each chunk\n",
    "    futures = [executor.submit(read_geopackage_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    gdfs = [future.result() for future in futures]\n",
    "\n",
    "# Concatenate the GeoDataFrames obtained from different chunks\n",
    "result_gdf = gpd.concat(gdfs, ignore_index=True)\n",
    "\n",
    "# Now 'result_gdf' contains the concatenated GeoDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161e2b1-bb26-4c43-9340-eb556827944e",
   "metadata": {},
   "source": [
    "# passage au format vecteur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81f34476-27b6-46fa-a339-f76eab5cfe8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "#poligonize the raster with float values\n",
    "\n",
    "#change the shadow_to_sun_time column\n",
    "gdf = gpd.read_file('start_sun_timestamp.gpkg')\n",
    "\n",
    "gdf['time'] = gdf['time'].mask(gdf['time'] < 0, np.nan)\n",
    "\n",
    "gdf['time'] = gdf['time'].apply(lambda x: datetime.fromtimestamp(x).replace(second=0).strftime('%Y-%m-%d %H:%M:%S') if not np.isnan(x) else \"\")\n",
    "\n",
    "gdf.dissolve('time').to_file('start_sun_timestamp_dissolved.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9dd49954-6aaa-4583-bba7-062be8ccf944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file('end_sun_timestamp.gpkg')\n",
    "\n",
    "gdf['time'] = gdf['time'].mask(gdf['time'] < 0, np.nan)\n",
    "\n",
    "gdf['time'] = gdf['time'].apply(lambda x: datetime.fromtimestamp(x).replace(second=0).strftime('%Y-%m-%d %H:%M:%S') if not np.isnan(x) else \"\")\n",
    "\n",
    "gdf.dissolve('time').to_file('end_sun_timestamp_dissolved.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e063d-e130-41f0-9d3d-477cacc836a0",
   "metadata": {},
   "source": [
    "# Intersection des deux rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5805219d-7fad-4453-b1cb-d1e5153fa4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/geodataframe.py:1803: FutureWarning: `unary_union` returned None due to all-None GeoSeries. In future, `unary_union` will return 'GEOMETRYCOLLECTION EMPTY' instead.\n",
      "  merged_geom = block.unary_union\n",
      "/tmp/slurm-3838576/ipykernel_1651519/3484396975.py:1: UserWarning: `keep_geom_type=True` in overlay resulted in 1334486 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  intersection = gpd.overlay(gpd.read_file('start_sun_timestamp_dissolved.gpkg'), gpd.read_file('end_sun_timestamp_dissolved.gpkg'), how='intersection')\n"
     ]
    }
   ],
   "source": [
    "intersection = gpd.overlay(gpd.read_file('start_sun_timestamp_dissolved.gpkg'), gpd.read_file('end_sun_timestamp_dissolved.gpkg'), how='intersection')\n",
    "\n",
    "intersection.to_file('intersection_start_end_sun.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4c2a29-82b6-4b49-b412-2cdef60989d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = gpd.read_file('intersection_start_end_sun.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0a250ab4-35f5-437b-ad02-49bcbfc888ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intersection['time_1'] = intersection['time_1'].apply(lambda x: '2024-05-31 23:59:59' if x == '1970-01-01 00:00:00' else x)\n",
    "intersection['time_2'] = intersection['time_2'].apply(lambda x: '2024-05-31 23:59:59' if x == '1970-01-01 00:00:00' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "10582a63-248c-4a4f-985e-e5f31c837b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection.drop(0).to_file('intersection_start_end_sun_rectified.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20e728-1b00-4adc-b493-fe6889f64714",
   "metadata": {},
   "source": [
    "# GIF animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4ff9272-96ea-45ec-8c8f-1b69f8601cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "# List of image file names\n",
    "image_files = sorted(glob.glob('carte_ensoleillement*.png'))[2:-1]\n",
    "print(len(image_files))\n",
    "# Open each image and append to a list\n",
    "images = []\n",
    "for filename in image_files:\n",
    "    images.append(Image.open(filename))\n",
    "\n",
    "# Save the images as an animated GIF\n",
    "images[0].save('animated.gif', save_all=True, append_images=images[1:], duration=1000, loop=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ffbd4-a09d-49bd-a740-a38076c64152",
   "metadata": {},
   "source": [
    "# filter small polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21534b8c-1801-49b9-afcb-d3bdcd3758c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple filter\n",
    "threshold = 2\n",
    "geom = intersection.iloc[13].geometry\n",
    "gpd.GeoDataFrame(geometry=[MultiPolygon([polygon for polygon in geom.geoms if polygon.area > threshold])]).to_file('multipoly_example.gpkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd32df1e-2f30-4ad4-bc26-1d977607a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convex hull before filter\n",
    "threshold = 2\n",
    "geom = intersection.iloc[13].geometry\n",
    "gpd.GeoDataFrame(geometry=[MultiPolygon([polygon.convex_hull for polygon in geom.geoms if polygon.area > threshold])]).to_file('multipoly_example_convex_hull.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e77cc0b0-6a66-4472-b2f1-c545a07de04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth before filter\n",
    "threshold = 2\n",
    "geom = intersection.iloc[13].geometry\n",
    "gpd.GeoDataFrame(geometry=[MultiPolygon([polygon.simplify(tolerance=0.3) for polygon in geom.geoms if polygon.area > threshold])]).to_file('multipoly_example_smoothen.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325f4e8-5206-41f9-86f4-3416141a2976",
   "metadata": {},
   "source": [
    "# Combinaison des deux rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08ef0cc8-21df-4625-a93a-17aca0769113",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters_stacked = np.dstack((start_sun, end_sun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d81e189-4810-4255-a17a-0673d6320782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364118"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.reshape(rasters_stacked, (-1, rasters_stacked.shape[2])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95d8804d-8812-4980-9f32-94594f0d6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.unique([rasters_stacked[i][i] for i in range(rasters_stacked.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b03e3fc1-9d8f-458d-b1fa-f873d91a84b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys[~np.isnan(keys).any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d515e-c526-42ef-baec-908d12d992e0",
   "metadata": {},
   "source": [
    "# Test de la technique quadruplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "185f0874-6198-480d-a651-2af582961dc3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1:\n",
      " [[2 0 1 2 1 2 2 2 2 2]\n",
      " [1 2 2 1 0 1 2 2 1 0]\n",
      " [2 2 2 1 0 0 1 2 0 0]\n",
      " [1 1 1 1 0 0 1 1 1 2]\n",
      " [2 1 0 1 0 0 2 0 0 0]\n",
      " [2 0 1 2 2 1 1 0 1 0]\n",
      " [1 0 2 2 1 0 0 1 0 2]\n",
      " [1 1 0 2 2 1 2 0 0 2]\n",
      " [2 0 1 1 2 1 2 2 0 0]\n",
      " [0 0 2 0 1 0 1 0 2 2]]\n",
      "\n",
      "Array 2:\n",
      " [[1 2 1 0 2 0 1 2 0 0]\n",
      " [0 0 1 2 1 2 0 0 0 1]\n",
      " [0 2 2 2 2 0 1 2 2 1]\n",
      " [2 0 1 2 2 0 1 2 2 1]\n",
      " [0 0 1 1 2 0 0 1 0 1]\n",
      " [1 1 1 0 2 0 1 2 0 2]\n",
      " [2 0 0 1 2 2 2 2 2 1]\n",
      " [2 2 0 1 0 0 2 1 2 2]\n",
      " [1 1 1 2 0 1 0 2 0 0]\n",
      " [1 1 2 2 0 2 0 1 1 0]]\n",
      "\n",
      "Array 3:\n",
      " [[2 1 1 1 1 2 2 0 2 0]\n",
      " [0 1 0 0 2 0 1 1 0 0]\n",
      " [1 1 2 2 2 1 2 1 1 0]\n",
      " [2 2 0 0 2 2 0 2 0 2]\n",
      " [0 1 0 0 0 1 0 0 0 2]\n",
      " [0 0 1 1 2 1 0 2 1 2]\n",
      " [2 2 1 2 1 1 2 2 1 0]\n",
      " [1 0 2 1 2 0 2 2 2 2]\n",
      " [2 1 1 0 0 1 2 0 1 2]\n",
      " [1 0 1 2 1 2 1 1 0 0]]\n",
      "\n",
      "Array 4:\n",
      " [[1 0 0 0 0 2 0 1 1 2]\n",
      " [0 1 1 2 2 0 0 2 2 1]\n",
      " [2 1 0 0 2 0 0 2 0 2]\n",
      " [0 1 2 0 0 1 0 1 1 2]\n",
      " [2 2 1 0 1 1 1 0 0 0]\n",
      " [1 0 2 1 1 1 2 0 1 1]\n",
      " [2 2 2 0 1 2 0 0 1 1]\n",
      " [2 1 2 2 1 0 1 2 0 0]\n",
      " [1 0 1 0 0 0 1 0 2 1]\n",
      " [1 1 2 1 2 1 2 1 0 1]]\n",
      "\n",
      "Stacked Array:\n",
      " [2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize four random arrays of size 10x10\n",
    "array1 = np.random.randint(0, 3, size=(10,10))\n",
    "array2 = np.random.randint(0, 3, size=(10,10))\n",
    "array3 = np.random.randint(0, 3, size=(10,10))\n",
    "array4 = np.random.randint(0, 3, size=(10,10))\n",
    "\n",
    "# Stack the arrays along a new axis (axis=2 in this case)\n",
    "stacked_array = np.stack([array1, array2, array3, array4], axis=2)\n",
    "\n",
    "# Display the individual arrays and the stacked array\n",
    "print(\"Array 1:\\n\", array1)\n",
    "print(\"\\nArray 2:\\n\", array2)\n",
    "print(\"\\nArray 3:\\n\", array3)\n",
    "print(\"\\nArray 4:\\n\", array4)\n",
    "\n",
    "print(\"\\nStacked Array:\\n\", stacked_array[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56d892da-72a9-4839-97fa-bba610ed25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coded_raster(stacked_array):\n",
    "    # reshape array\n",
    "    data_2d = stacked_array.reshape(-1, 4)\n",
    "\n",
    "    # get quadruples\n",
    "    unique_quadruplets, _ = np.unique(data_2d, axis=0, return_inverse=True)\n",
    "    \n",
    "    # create dict\n",
    "    quadruplet_dict = {tuple(q): i + 1 for i, q in enumerate(unique_quadruplets)}\n",
    "    \n",
    "    # create coded raster\n",
    "    new_array = np.array([[quadruplet_dict[tuple(stacked_array[i, j, :])] for j in range(stacked_array.shape[1])] for i in range(stacked_array.shape[0])])\n",
    "\n",
    "    return new_array, quadruplet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf86e06-b115-4d7a-9826-d3f93f3ff46c",
   "metadata": {},
   "source": [
    "# Improving dissolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e77be86-de87-414f-ada4-000fffa7782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = gpd.read_file(\"coded_geoms.gpkg\", chunksize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "73ad164a-1006-4e8a-851e-728b5ead5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved = gpd.read_file(\"coded_geoms_dissolved.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a636-5f35-42b1-a2a5-1985dcc89950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "65abb467-2b2b-44b4-9078-4f3007129b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing \n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "75c0b988-4b20-46c6-81c5-5a5bb065780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = dissolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6622dc65-6858-4810-8e6c-abff8ab8998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnary = {v: k for k, v in dictionnary.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "62d9713d-e81a-43ae-906f-9315bba371aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour récupérer le quadruplé à partir du dictionnaire\n",
    "def get_quadruplet(code):\n",
    "    return pd.Series(dictionnary.get(code, [None, None, None, None]), index=['first_sun_appearance', 'first_shadow_appearance', 'second_sun_appearance', 'second_shadow_appearance'])\n",
    "\n",
    "# Appliquer la fonction à la colonne 'code'\n",
    "chunk[['first_sun_appearance', 'first_shadow_appearance', 'second_sun_appearance', 'second_shadow_appearance']] = chunk['code'].apply(get_quadruplet)\n",
    "\n",
    "chunk = chunk.drop(columns='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7aa13b9-16dc-4941-ad49-9ac5f3ad0223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2024, 5, 31, 8, 0),\n",
       " datetime.datetime(2024, 5, 31, 8, 30),\n",
       " datetime.datetime(2024, 5, 31, 9, 0),\n",
       " datetime.datetime(2024, 5, 31, 9, 30),\n",
       " datetime.datetime(2024, 5, 31, 10, 0),\n",
       " datetime.datetime(2024, 5, 31, 10, 30),\n",
       " datetime.datetime(2024, 5, 31, 11, 0),\n",
       " datetime.datetime(2024, 5, 31, 11, 30),\n",
       " datetime.datetime(2024, 5, 31, 12, 0),\n",
       " datetime.datetime(2024, 5, 31, 12, 30),\n",
       " datetime.datetime(2024, 5, 31, 13, 0),\n",
       " datetime.datetime(2024, 5, 31, 13, 30),\n",
       " datetime.datetime(2024, 5, 31, 14, 0),\n",
       " datetime.datetime(2024, 5, 31, 14, 30),\n",
       " datetime.datetime(2024, 5, 31, 15, 0),\n",
       " datetime.datetime(2024, 5, 31, 15, 30),\n",
       " datetime.datetime(2024, 5, 31, 16, 0),\n",
       " datetime.datetime(2024, 5, 31, 16, 30),\n",
       " datetime.datetime(2024, 5, 31, 17, 0),\n",
       " datetime.datetime(2024, 5, 31, 17, 30),\n",
       " datetime.datetime(2024, 5, 31, 18, 0),\n",
       " datetime.datetime(2024, 5, 31, 18, 30),\n",
       " datetime.datetime(2024, 5, 31, 19, 0),\n",
       " datetime.datetime(2024, 5, 31, 19, 30),\n",
       " datetime.datetime(2024, 5, 31, 20, 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa40c9a-1cec-40f8-8f87-ecdcb95ebbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_times = gpd.read_file('../outputs/JO/sun_times.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06895edf-bce5-47bb-8900-39ce20b6e5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sun_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "311fec46-a263-4cec-a455-e7a4ad5aed94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['first_sun_appearance', 'first_shadow_appearance', 'second_sun_appearance', 'second_shadow_appearance']\n",
    "sun_times[cols] = sun_times[cols].applymap(lambda x: pd.to_datetime(x, unit='s') if x > 0 else np.nan if x == -1 else times[0].replace(hour=23, minute=59, second=59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fba6f4b-9d59-442d-8615-a5db7983f7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sec_to_hms(t):\n",
    "    h = int(t // 3600)\n",
    "    m = int((t - h * 3600) // 60)\n",
    "    s = t - h * 3600 - m * 60\n",
    "\n",
    "    h = \"0\" + str(h) if h < 10 else h\n",
    "    m = \"0\" + str(m) if m < 10 else m\n",
    "    s = \"0\" + str(s) if s < 10 else s\n",
    "\n",
    "    return \"{}:{}:{:.3}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38cd44b-e3e4-4528-82d6-549fa83e19eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:01:05.78\n"
     ]
    }
   ],
   "source": [
    "def seconds_to_hhmmss(seconds):\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return '{:02d}:{:02d}:{:02d}.{:02d}'.format(int(hours), int(minutes), int(seconds), int((seconds - int(seconds)) * 100))\n",
    "\n",
    "\n",
    "total_seconds = 3665.7894494  # Replace with the actual duration in seconds\n",
    "formatted_time = seconds_to_hhmmss(total_seconds)\n",
    "print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e25f59b-b2ef-47f7-a3a3-88fc653c269d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 4], [7, 10], [13, 16]], [[2, 5], [8, 11], [14, 17]], [[3, 6], [9, 12], [15, 18]]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming your original 3D list is called original_list\n",
    "original_list = [\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[7, 8, 9], [10, 11, 12]],\n",
    "    [[13, 14, 15], [16, 17, 18]]\n",
    "]\n",
    "\n",
    "# Inverting dimensions 1 and 2\n",
    "inverted_list = [[[original_list[i][j][k] for j in range(len(original_list[i]))] for i in range(len(original_list))] for k in range(len(original_list[0][0]))]\n",
    "\n",
    "# Print the inverted list\n",
    "print(inverted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be69f3-7e2f-48da-a8e2-946eeb1c4a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f906e1-6385-43c4-9ce6-7f23e2934327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your list is called nested_list\n",
    "import itertools\n",
    "# Flatten the first dimension\n",
    "flattened_list = list(itertools.chain.from_iterable(original_list))\n",
    "\n",
    "# Print the flattened list\n",
    "print(flattened_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7dcbe-2c33-434f-8741-47f54fcc8062",
   "metadata": {},
   "source": [
    "# reduce mask storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408ee97c-2585-4647-94ec-5e5bf929730c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected array of dim 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Set the GeoTransform (optional)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# bitmask_ds.SetGeoTransform(...)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Write the data to the band\u001b[39;00m\n\u001b[1;32m     21\u001b[0m bitmask_band \u001b[38;5;241m=\u001b[39m bitmask_ds\u001b[38;5;241m.\u001b[39mGetRasterBand(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mbitmask_band\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Close the dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m bitmask_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/osgeo/gdal.py:4090\u001b[0m, in \u001b[0;36mBand.WriteArray\u001b[0;34m(self, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mWriteArray\u001b[39m(\u001b[38;5;28mself\u001b[39m, array, xoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, yoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   4085\u001b[0m                resample_alg\u001b[38;5;241m=\u001b[39mgdalconst\u001b[38;5;241m.\u001b[39mGRIORA_NearestNeighbour,\n\u001b[1;32m   4086\u001b[0m                callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4087\u001b[0m                callback_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gdal_array\n\u001b[0;32m-> 4090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgdal_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBandWriteArray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mresample_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample_alg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4092\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4093\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallback_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/osgeo/gdal_array.py:510\u001b[0m, in \u001b[0;36mBandWriteArray\u001b[0;34m(band, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pure python implementation of writing a chunk of a GDAL file\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03mfrom a numpy array.  Used by the gdal.Band.WriteArray method.\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(array\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected array of dim 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m xsize \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    513\u001b[0m ysize \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: expected array of dim 2"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "import rasterio\n",
    "# Assuming 'mask_data' is your binary mask raster with values 0 and 1\n",
    "\n",
    "with rasterio.open('../../outputs/JO/test_bytes/75-2021-0653-6866-LA93-0M50-hillshade-20240726-1100.tif') as src:\n",
    "        # Read the image data\n",
    "        mask_data = src.read()\n",
    "        \n",
    "# Define the raster dimensions\n",
    "width = mask_data.shape[1]\n",
    "height = mask_data.shape[0]\n",
    "\n",
    "# Create a new GeoTIFF file\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "bitmask_ds = driver.Create('../../outputs/JO/test_bytes/mask.tif', width, height, 1, gdal.GDT_Byte)\n",
    "\n",
    "# Set the GeoTransform (optional)\n",
    "# bitmask_ds.SetGeoTransform(...)\n",
    "\n",
    "# Write the data to the band\n",
    "bitmask_band = bitmask_ds.GetRasterBand(1)\n",
    "bitmask_band.WriteArray(mask_data)\n",
    "\n",
    "# Close the dataset\n",
    "bitmask_ds = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
